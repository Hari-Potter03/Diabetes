---
title: "Diabetes"
author: "Srihari Srinivasan"
date: "2023-05-14"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(tinytex)
library(ROCR)
library(caret)

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(out.width = "60%")
```

# Importing Data

```{r}
df <- read.csv('/Users/sriharisrinivasan/Documents/GitHub/Diabetes/diabetes.csv')
df
```

# Exploratory Analysis

```{r}
str(df)
```
## Graphs

```{r}
df %>% ggplot(aes(Age, Outcome)) +
  geom_jitter(width=0,height=0.1) +
  geom_smooth(method='glm',se=FALSE,method.args=list(family='binomial')) +
  scale_y_continuous(limits=c(0,1),breaks=seq(0,1,0.1)) +
  theme_bw()
```
# Multi-Collinearity

```{r}
cor(df)
```


# Modeling

```{r}
set.seed(1)

model <- glm(data = df, Outcome ~., family = "binomial") 
coefficients(summary(model))

rows <- sample(x=nrow(df),size=floor(0.75*nrow(df)),replace=FALSE)
training <- df[rows,]
testing <- df[-rows,]
```
## Backwards-Stepwise Selection Method

```{r}
#create empty vector to hold accuracies
accuracy <- data.frame(class_acc=rep(NA,8))

#remove each variable and save accuracy
for(i in 1:8){
  
  #remove i-th variable
  training_min1 <- training[,-i]
  
  #fit model minus one variable
  model_min1 <- glm(Outcome~.,data=training_min1,family='binomial')
  
  #classify testing observations
  testing_class_min1 <- testing %>%
    mutate(Outcome_prob=predict(model_min1,newdata=testing,type='response'),
           Outcome_class=if_else(Outcome_prob>0.5,1,0))
  
  #compute and save overall accuracy
  accuracy$class_acc[i] <- mean(testing_class_min1$Outcome==testing_class_min1$Outcome_class)
}
```

```{r}
#permanently remove one variable
training_min1 <- training[,-6]

#create empty vector to hold accuracies
accuracy2 <- data.frame(class_acc=rep(NA,7))

#remove each variable and save accuracy
for(i in 1:7){
  
  #remove i-th variable
  training_min2 <- training_min1[,-i]
  
  #fit model minus one variable
  model_min2 <- glm(Outcome~.,data=training_min2,family='binomial')
  
  #classify testing observations
  testing_class_min2 <- testing %>%
    mutate(Outcome_prob=predict(model_min2,newdata=testing,type='response'),
           Outcome_class=if_else(Outcome_prob>0.5,1,0))
  
  #compute and save overall accuracy
  accuracy2$class_acc[i] <- mean(testing_class_min2$Outcome==testing_class_min2$Outcome_class)
}
```

```{r}
#permanently remove one variable
training_min2 <- training_min1[,-6]

#create empty vector to hold accuracies
accuracy3 <- data.frame(class_acc=rep(NA,6))

#remove each variable and save accuracy
for(i in 1:6){
  
  #remove i-th variable
  training_min3 <- training_min2[,-i]
  
  #fit model minus one variable
  model_min3 <- glm(Outcome~.,data=training_min3,family='binomial')
  
  #classify testing observations
  testing_class_min3 <- testing %>%
    mutate(Outcome_prob=predict(model_min3,newdata=testing,type='response'),
           Outcome_class=if_else(Outcome_prob>0.5,1,0))
  
  #compute and save overall accuracy
  accuracy3$class_acc[i] <- mean(testing_class_min3$Outcome==testing_class_min3$Outcome_class)
}
```

```{r}
#permanently remove one variable
training_min3 <- training_min2[,-5]

#create empty vector to hold accuracies
accuracy4 <- data.frame(class_acc=rep(NA,5))

#remove each variable and save accuracy
for(i in 1:5){
  
  #remove i-th variable
  training_min4 <- training_min3[,-i]
  
  #fit model minus one variable
  model_min4 <- glm(Outcome~.,data=training_min4,family='binomial')
  
  #classify testing observations
  testing_class_min4 <- testing %>%
    mutate(Outcome_prob=predict(model_min4,newdata=testing,type='response'),
           Outcome_class=if_else(Outcome_prob>0.5,1,0))
  
  #compute and save overall accuracy
  accuracy4$class_acc[i] <- mean(testing_class_min4$Outcome==testing_class_min4$Outcome_class)
}
```

```{r}
#permanently remove one variable
training_min4 <- training_min3[,-4]

#create empty vector to hold accuracies
accuracy5 <- data.frame(class_acc=rep(NA,4))

#remove each variable and save accuracy
for(i in 1:4){
  
  #remove i-th variable
  training_min5 <- training_min4[,-i]
  
  #fit model minus one variable
  model_min5 <- glm(Outcome~.,data=training_min5,family='binomial')
  
  #classify testing observations
  testing_class_min5 <- testing %>%
    mutate(Outcome_prob=predict(model_min5,newdata=testing,type='response'),
           Outcome_class=if_else(Outcome_prob>0.5,1,0))
  
  #compute and save overall accuracy
  accuracy5$class_acc[i] <- mean(testing_class_min5$Outcome==testing_class_min5$Outcome_class)
}
```

## Final Model

```{r}
training <- training_min4
testing <- testing %>% select(Pregnancies, Glucose, BloodPressure, Age, Outcome)

model_final <- glm(Outcome~., data=training, family = "binomial")
coefficients(summary(model_final))

testing_class <- testing %>%
  mutate(Outcome_prob=predict(model_final,newdata=testing,type='response'),
         Outcome_class=if_else(Outcome_prob>0.5,1,0))

#compute overall testing accuracy
mean(testing_class$Outcome==testing_class$Outcome_class)
```
## ROC Curve

```{r}
#create required objects for ROC
pred <- prediction(predictions=testing_class$Outcome_prob,
                   labels=testing_class$Outcome)

perf <- performance(pred,measure='sens',x.measure='spec')

#plot ROC curve with threshold color
plot(perf,colorize=TRUE)
```
## Accuracy vs. Cutoff

```{r}
#plot accuracy curve
perf2 <- performance(pred,measure='acc')
plot(perf2)
```
## Area Under the Curve

```{r}
#plot TPR vs. FPR curve
perf3 <- performance(pred,measure='tpr',x.measure='fpr')
plot(perf3,colorize=TRUE)
abline(a=0,b=1)
```

```{r}
#compute AUC for the ROC curve
perf4 <- performance(pred,measure='auc')
perf4@y.values
```

## Changing Threshold -> Finalized Model

```{r}
model_final <- glm(Outcome~., data=training, family = "binomial")
coefficients(summary(model_final))

# Change the threshold to 0.54
testing_class <- testing %>%
  mutate(Outcome_prob=predict(model_final,newdata=testing,type='response'),
         Outcome_class=if_else(Outcome_prob>0.54,1,0))

#compute overall testing accuracy
mean(testing_class$Outcome==testing_class$Outcome_class)
```
## Sensitivity vs Specificity

```{r}
# create contingency table
contingency_table <- table(testing_class$Outcome,testing_class$Outcome_class)

# Calculate specificity
specificity <- contingency_table[1, 1] / (contingency_table[1, 1] + contingency_table[1, 2])

# Calculate sensitivity
sensitivity <- contingency_table[2, 2] / (contingency_table[2, 2] + contingency_table[2, 1])

# Print the results
print(paste("Specificity:", specificity))
print(paste("Sensitivity:", sensitivity))
```

# Cross-Validation (k-fold) 

## Creating Samples

We will do 4 folds. 

```{r}
# Set the seed for reproducibility
set.seed(1)

# Create four mutually exclusive samples
num_samples <- 4

# Create empty list to store the samples
sample_list <- vector("list", num_samples)

# Split the dataset into mutually exclusive samples
sample_indices <- createDataPartition(df$Outcome, p = 0.25, times = num_samples)

for (i in 1:num_samples) {
  sample_list[[i]] <- df[sample_indices[[i]], ]
}

# Access the samples
A <- sample_list[[1]] %>% select(Pregnancies, Glucose, BloodPressure, Age, Outcome)
B <- sample_list[[2]] %>% select(Pregnancies, Glucose, BloodPressure, Age, Outcome)
C <- sample_list[[3]] %>% select(Pregnancies, Glucose, BloodPressure, Age, Outcome)
D <- sample_list[[4]] %>% select(Pregnancies, Glucose, BloodPressure, Age, Outcome)
```

## Validation Testing

### Train on B,C,D and test on A

```{r}
train_1 <- rbind(B, C, D)

model_A <- glm(Outcome~., data=train_1, family = "binomial")

# Change the threshold to 0.54
testing_class <- A %>%
  mutate(Outcome_prob=predict(model_A,newdata=A,type='response'),
         Outcome_class=if_else(Outcome_prob>0.54,1,0))

#compute overall testing accuracy
mean(testing_class$Outcome==testing_class$Outcome_class)
```

### Train on A,C,D and test on B

```{r}
train_2 <- rbind(A, C, D)

model_B <- glm(Outcome~., data=train_2, family = "binomial")

# Change the threshold to 0.54
testing_class <- B %>%
  mutate(Outcome_prob=predict(model_B,newdata=B,type='response'),
         Outcome_class=if_else(Outcome_prob>0.54,1,0))

#compute overall testing accuracy
mean(testing_class$Outcome==testing_class$Outcome_class)
```

### Train on A,B,D and test on C

```{r}
train_3 <- rbind(A, B, D)

model_C <- glm(Outcome~., data=train_3, family = "binomial")

# Change the threshold to 0.54
testing_class <- C %>%
  mutate(Outcome_prob=predict(model_C,newdata=C,type='response'),
         Outcome_class=if_else(Outcome_prob>0.54,1,0))

#compute overall testing accuracy
mean(testing_class$Outcome==testing_class$Outcome_class)
```

### Train on A,B,C and test on D

```{r}
train_4 <- rbind(A, B, C)

model_D <- glm(Outcome~., data=train_4, family = "binomial")

# Change the threshold to 0.54
testing_class <- D %>%
  mutate(Outcome_prob=predict(model_D,newdata=D,type='response'),
         Outcome_class=if_else(Outcome_prob>0.54,1,0))

#compute overall testing accuracy
mean(testing_class$Outcome==testing_class$Outcome_class)
```